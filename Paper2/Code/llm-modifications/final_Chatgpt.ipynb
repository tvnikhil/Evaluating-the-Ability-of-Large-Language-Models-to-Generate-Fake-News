{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "huV4Ln6La1lR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1732829641266,
     "user": {
      "displayName": "Research",
      "userId": "04143786625875785090"
     },
     "user_tz": 360
    },
    "id": "huV4Ln6La1lR",
    "outputId": "c4bc1451-6ccf-44da-8262-5699e160dfd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f2e5bb-cdf1-4b6f-9c8c-4b560ea6e8b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1732829641266,
     "user": {
      "displayName": "Research",
      "userId": "04143786625875785090"
     },
     "user_tz": 360
    },
    "id": "14f2e5bb-cdf1-4b6f-9c8c-4b560ea6e8b0",
    "outputId": "10f64fa0-2fca-4eef-9622-84af7ecc5966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY is set: 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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"xxx\"\n",
    "print(\"OPENAI_API_KEY is set:\", os.environ.get(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4ded986-82b3-477c-8158-def848273a20",
   "metadata": {
    "executionInfo": {
     "elapsed": 168,
     "status": "ok",
     "timestamp": 1732829641432,
     "user": {
      "displayName": "Research",
      "userId": "04143786625875785090"
     },
     "user_tz": 360
    },
    "id": "f4ded986-82b3-477c-8158-def848273a20"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, ValidationError\n",
    "import json\n",
    "\n",
    "class Article(BaseModel):\n",
    "    clean_title: str\n",
    "    label: int\n",
    "\n",
    "class ArticleArray(BaseModel):\n",
    "    articles: list[Article]\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ZLQSLwHRffq0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4321,
     "status": "ok",
     "timestamp": 1732829645751,
     "user": {
      "displayName": "Research",
      "userId": "04143786625875785090"
     },
     "user_tz": 360
    },
    "id": "ZLQSLwHRffq0",
    "outputId": "ba2691bb-7814-44d6-e74c-913a0d8feaa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: json_repair in /usr/local/lib/python3.10/dist-packages (0.30.2)\n"
     ]
    }
   ],
   "source": [
    "pip install json_repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "JC3n_ZlPYQBL",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732829645751,
     "user": {
      "displayName": "Research",
      "userId": "04143786625875785090"
     },
     "user_tz": 360
    },
    "id": "JC3n_ZlPYQBL"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "file_path = \"/content/drive/MyDrive/Final project/checkTest.tsv\"  # Replace with your file path\n",
    "batch_size = 50  # Adjust the batch size as needed\n",
    "start_batch = 0  # Specify the starting batch if resuming\n",
    "OUTPUT_FILE = \"/content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\"\n",
    "SAVE_INTERVAL = 5  # Save progress every 5 iterations\n",
    "\n",
    "# Load the .tsv file\n",
    "data = pd.read_csv(file_path, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ixbirHZYRQA",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732829645751,
     "user": {
      "displayName": "Research",
      "userId": "04143786625875785090"
     },
     "user_tz": 360
    },
    "id": "2ixbirHZYRQA"
   },
   "outputs": [],
   "source": [
    "sysInstr = \"\"\"I will provide article titles in JSON format, each structured across multiple fields. Your task is to subtly modify the `clean_title` to retain its original intent and meaning while making the titles more challenging for fake news detection models, including BERT, CNN, Bi-LSTM, and SVM.\n",
    "\n",
    "**Modification Guidelines:**\n",
    "- **Synonym Variation:** Replace words with less common synonyms or alternative expressions to alter surface-level patterns while preserving meaning.\n",
    "- **Structural Changes:** Rephrase the title by altering the sentence structure (e.g., switching clauses or reordering phrases).\n",
    "- **Contextual Nuance:** Introduce subtle hints of sarcasm, irony, or ambiguity to challenge the model's ability to infer context accurately.\n",
    "- **Colloquial Language:** Use idiomatic expressions, informal phrases, or region-specific terms to increase linguistic variability.\n",
    "- **Negation and Sentiment Shifts:** Add subtle negations, expressions of doubt, or shifts in sentiment to complicate sentiment analysis.\n",
    "- **Noise Injection:** Incorporate harmless but unusual punctuation, slight spelling variations, or uncommon phrasing to disrupt token patterns.\n",
    "\n",
    "**Constraints:**\n",
    "- Do not alter the category or core meaning of the title.\n",
    "- The title must remain natural, plausible, and coherent, as it would appear in a reliable source.\n",
    "- The modified titles should challenge model detection by leveraging long-range dependencies, local context changes, and subtle semantic shifts.\n",
    "- Ensure the response is in valid JSON format, where each row includes `clean_title` and `label`.\n",
    "- Avoid providing introductions, explanations, or extraneous text. Do not add unnecessary start or stop characters.\n",
    "\n",
    "Your modifications should aim to evade detection by exploiting weaknesses in the models' ability to process context, handle nuanced language, and recognize long-range dependencies.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "u0YYETZtYZAt",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1732829645751,
     "user": {
      "displayName": "Research",
      "userId": "04143786625875785090"
     },
     "user_tz": 360
    },
    "id": "u0YYETZtYZAt"
   },
   "outputs": [],
   "source": [
    "# Load existing data if resuming\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    with open(OUTPUT_FILE, \"r\") as f:\n",
    "        modified_data = json.load(f)\n",
    "    print(f\"Resuming: Loaded {len(modified_data)} entries from {OUTPUT_FILE}\")\n",
    "else:\n",
    "    modified_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "LeUtw--XYbuL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1469775,
     "status": "ok",
     "timestamp": 1732831115522,
     "user": {
      "displayName": "Research",
      "userId": "04143786625875785090"
     },
     "user_tz": 360
    },
    "id": "LeUtw--XYbuL",
    "outputId": "b1f25722-4d03-419a-8538-9c185c570a7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1 (Rows 0 to 49)...\n",
      "Processing batch 2 (Rows 50 to 99)...\n",
      "Processing batch 3 (Rows 100 to 149)...\n",
      "Processing batch 4 (Rows 150 to 199)...\n",
      "Processing batch 5 (Rows 200 to 249)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 6 (Rows 250 to 299)...\n",
      "Processing batch 7 (Rows 300 to 349)...\n",
      "Processing batch 8 (Rows 350 to 399)...\n",
      "Processing batch 9 (Rows 400 to 449)...\n",
      "Processing batch 10 (Rows 450 to 499)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 11 (Rows 500 to 549)...\n",
      "Processing batch 12 (Rows 550 to 599)...\n",
      "Processing batch 13 (Rows 600 to 649)...\n",
      "Processing batch 14 (Rows 650 to 699)...\n",
      "Processing batch 15 (Rows 700 to 749)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 16 (Rows 750 to 799)...\n",
      "Processing batch 17 (Rows 800 to 849)...\n",
      "Processing batch 18 (Rows 850 to 899)...\n",
      "Processing batch 19 (Rows 900 to 949)...\n",
      "Processing batch 20 (Rows 950 to 999)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 21 (Rows 1000 to 1049)...\n",
      "Processing batch 22 (Rows 1050 to 1099)...\n",
      "Processing batch 23 (Rows 1100 to 1149)...\n",
      "Processing batch 24 (Rows 1150 to 1199)...\n",
      "Processing batch 25 (Rows 1200 to 1249)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 26 (Rows 1250 to 1299)...\n",
      "Processing batch 27 (Rows 1300 to 1349)...\n",
      "Processing batch 28 (Rows 1350 to 1399)...\n",
      "Processing batch 29 (Rows 1400 to 1449)...\n",
      "Processing batch 30 (Rows 1450 to 1499)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 31 (Rows 1500 to 1549)...\n",
      "Processing batch 32 (Rows 1550 to 1599)...\n",
      "Processing batch 33 (Rows 1600 to 1649)...\n",
      "Processing batch 34 (Rows 1650 to 1699)...\n",
      "Processing batch 35 (Rows 1700 to 1749)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 36 (Rows 1750 to 1799)...\n",
      "Processing batch 37 (Rows 1800 to 1849)...\n",
      "Processing batch 38 (Rows 1850 to 1899)...\n",
      "Processing batch 39 (Rows 1900 to 1949)...\n",
      "Processing batch 40 (Rows 1950 to 1999)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 41 (Rows 2000 to 2049)...\n",
      "Processing batch 42 (Rows 2050 to 2099)...\n",
      "Processing batch 43 (Rows 2100 to 2149)...\n",
      "Processing batch 44 (Rows 2150 to 2199)...\n",
      "Processing batch 45 (Rows 2200 to 2249)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 46 (Rows 2250 to 2299)...\n",
      "Processing batch 47 (Rows 2300 to 2349)...\n",
      "Processing batch 48 (Rows 2350 to 2399)...\n",
      "Processing batch 49 (Rows 2400 to 2449)...\n",
      "Processing batch 50 (Rows 2450 to 2499)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 51 (Rows 2500 to 2549)...\n",
      "Processing batch 52 (Rows 2550 to 2599)...\n",
      "Processing batch 53 (Rows 2600 to 2649)...\n",
      "Processing batch 54 (Rows 2650 to 2699)...\n",
      "Processing batch 55 (Rows 2700 to 2749)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 56 (Rows 2750 to 2799)...\n",
      "Processing batch 57 (Rows 2800 to 2849)...\n",
      "Processing batch 58 (Rows 2850 to 2899)...\n",
      "Processing batch 59 (Rows 2900 to 2949)...\n",
      "Processing batch 60 (Rows 2950 to 2999)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 61 (Rows 3000 to 3049)...\n",
      "Processing batch 62 (Rows 3050 to 3099)...\n",
      "Processing batch 63 (Rows 3100 to 3149)...\n",
      "Processing batch 64 (Rows 3150 to 3199)...\n",
      "Processing batch 65 (Rows 3200 to 3249)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 66 (Rows 3250 to 3299)...\n",
      "Processing batch 67 (Rows 3300 to 3349)...\n",
      "Processing batch 68 (Rows 3350 to 3399)...\n",
      "Processing batch 69 (Rows 3400 to 3449)...\n",
      "Processing batch 70 (Rows 3450 to 3499)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 71 (Rows 3500 to 3549)...\n",
      "Processing batch 72 (Rows 3550 to 3599)...\n",
      "Processing batch 73 (Rows 3600 to 3649)...\n",
      "Processing batch 74 (Rows 3650 to 3699)...\n",
      "Processing batch 75 (Rows 3700 to 3749)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 76 (Rows 3750 to 3799)...\n",
      "Processing batch 77 (Rows 3800 to 3849)...\n",
      "Processing batch 78 (Rows 3850 to 3899)...\n",
      "Processing batch 79 (Rows 3900 to 3949)...\n",
      "Processing batch 80 (Rows 3950 to 3999)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 81 (Rows 4000 to 4049)...\n",
      "Processing batch 82 (Rows 4050 to 4099)...\n",
      "Processing batch 83 (Rows 4100 to 4149)...\n",
      "Processing batch 84 (Rows 4150 to 4199)...\n",
      "Processing batch 85 (Rows 4200 to 4249)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 86 (Rows 4250 to 4299)...\n",
      "Processing batch 87 (Rows 4300 to 4349)...\n",
      "Processing batch 88 (Rows 4350 to 4399)...\n",
      "Processing batch 89 (Rows 4400 to 4449)...\n",
      "Processing batch 90 (Rows 4450 to 4499)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 91 (Rows 4500 to 4549)...\n",
      "Processing batch 92 (Rows 4550 to 4599)...\n",
      "Processing batch 93 (Rows 4600 to 4649)...\n",
      "Processing batch 94 (Rows 4650 to 4699)...\n",
      "Processing batch 95 (Rows 4700 to 4749)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 96 (Rows 4750 to 4799)...\n",
      "Processing batch 97 (Rows 4800 to 4849)...\n",
      "Processing batch 98 (Rows 4850 to 4899)...\n",
      "Processing batch 99 (Rows 4900 to 4949)...\n",
      "Processing batch 100 (Rows 4950 to 4999)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 101 (Rows 5000 to 5049)...\n",
      "Processing batch 102 (Rows 5050 to 5099)...\n",
      "Processing batch 103 (Rows 5100 to 5149)...\n",
      "Processing batch 104 (Rows 5150 to 5199)...\n",
      "Processing batch 105 (Rows 5200 to 5249)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 106 (Rows 5250 to 5299)...\n",
      "Processing batch 107 (Rows 5300 to 5349)...\n",
      "Processing batch 108 (Rows 5350 to 5399)...\n",
      "Processing batch 109 (Rows 5400 to 5449)...\n",
      "Processing batch 110 (Rows 5450 to 5499)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 111 (Rows 5500 to 5549)...\n",
      "Processing batch 112 (Rows 5550 to 5599)...\n",
      "Processing batch 113 (Rows 5600 to 5649)...\n",
      "Processing batch 114 (Rows 5650 to 5699)...\n",
      "Processing batch 115 (Rows 5700 to 5749)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 116 (Rows 5750 to 5799)...\n",
      "Processing batch 117 (Rows 5800 to 5849)...\n",
      "Processing batch 118 (Rows 5850 to 5899)...\n",
      "Processing batch 119 (Rows 5900 to 5949)...\n",
      "Processing batch 120 (Rows 5950 to 5999)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 121 (Rows 6000 to 6049)...\n",
      "Processing batch 122 (Rows 6050 to 6099)...\n",
      "Processing batch 123 (Rows 6100 to 6149)...\n",
      "Processing batch 124 (Rows 6150 to 6199)...\n",
      "Processing batch 125 (Rows 6200 to 6249)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 126 (Rows 6250 to 6299)...\n",
      "Processing batch 127 (Rows 6300 to 6349)...\n",
      "Processing batch 128 (Rows 6350 to 6399)...\n",
      "Processing batch 129 (Rows 6400 to 6449)...\n",
      "Processing batch 130 (Rows 6450 to 6499)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 131 (Rows 6500 to 6549)...\n",
      "Processing batch 132 (Rows 6550 to 6599)...\n",
      "Processing batch 133 (Rows 6600 to 6649)...\n",
      "Processing batch 134 (Rows 6650 to 6699)...\n",
      "Processing batch 135 (Rows 6700 to 6749)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n",
      "Processing batch 136 (Rows 6750 to 6799)...\n",
      "Processing batch 137 (Rows 6800 to 6849)...\n",
      "Processing batch 138 (Rows 6850 to 6899)...\n",
      "Processing batch 139 (Rows 6900 to 6949)...\n",
      "Processing batch 140 (Rows 6950 to 6992)...\n",
      "Intermediate data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n"
     ]
    }
   ],
   "source": [
    "# Batch processing loop\n",
    "for iter, i in enumerate(range(start_batch * batch_size, len(data), batch_size), start=start_batch):\n",
    "    batch_df = data.iloc[i : i + batch_size]\n",
    "    print(f\"Processing batch {iter + 1} (Rows {i} to {min(i + batch_size - 1, len(data) - 1)})...\")\n",
    "\n",
    "    # Convert the batch DataFrame to JSON\n",
    "    batch_json = batch_df.to_json(orient=\"records\")\n",
    "\n",
    "    # Create the prompt for the batch\n",
    "    prompt = f\"\"\"\n",
    "    Modify the `clean_title` for the articles provided below while keeping `label` unchanged.\n",
    "    Articles (JSON):\n",
    "    {batch_json}\n",
    "    \"\"\"\n",
    "    prompt = sysInstr + \"\\n\" + prompt\n",
    "\n",
    "    try:\n",
    "        # Call OpenAI API to generate modified data\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0.8,\n",
    "            top_p=0.85,  # Adding the top_p parameter here\n",
    "            max_completion_tokens=8192,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sysInstr},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            response_format=ArticleArray  # Assuming ArticleArray is a valid format\n",
    "        )\n",
    "        output = completion.choices[0].message\n",
    "\n",
    "        if output.parsed:\n",
    "            res = output.parsed\n",
    "            for article in res.articles:\n",
    "                modified_data.append(article.model_dump())  # Append parsed articles\n",
    "        else:\n",
    "            raise ValueError(\"Invalid response format.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process batch {iter + 1}: {e}\")\n",
    "\n",
    "    # Save progress every SAVE_INTERVAL iterations\n",
    "    if (iter + 1) % SAVE_INTERVAL == 0:\n",
    "        with open(OUTPUT_FILE, \"w\") as json_file:\n",
    "            json.dump(modified_data, json_file, indent=4)\n",
    "        print(f\"Intermediate data saved to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "jBV11lkPYf31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1732831115523,
     "user": {
      "displayName": "Research",
      "userId": "04143786625875785090"
     },
     "user_tz": 360
    },
    "id": "jBV11lkPYf31",
    "outputId": "2f3fc902-8a9e-445d-d998-e25e606cdadf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final modified data saved to /content/drive/MyDrive/Final project/modified_data_chatgpt_max.json\n"
     ]
    }
   ],
   "source": [
    "# Save the final output\n",
    "with open(OUTPUT_FILE, \"w\") as json_file:\n",
    "    json.dump(modified_data, json_file, indent=4)\n",
    "print(f\"Final modified data saved to {OUTPUT_FILE}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
